{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GNR",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cedro3/GANsNRoses/blob/main/GNR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1mlTq7CzTnz"
      },
      "source": [
        "# セットアップ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41pPVq8uIe_b"
      },
      "source": [
        "# githubからコードを取得\n",
        "!git clone https://github.com/cedro3/GANsNRoses.git\n",
        "%cd GANsNRoses\n",
        "!pip install tqdm gdown kornia scipy opencv-python dlib moviepy lpips aubio ninja\n",
        "\n",
        "\n",
        "# ライブラリーのインポート\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils import data\n",
        "from torchvision import transforms, utils\n",
        "from tqdm import tqdm\n",
        "torch.backends.cudnn.benchmark = True\n",
        "import copy\n",
        "from util import *\n",
        "from PIL import Image\n",
        "\n",
        "from model import *\n",
        "import moviepy.video.io.ImageSequenceClip\n",
        "import scipy\n",
        "import cv2\n",
        "import dlib\n",
        "import kornia.augmentation as K\n",
        "from aubio import tempo, source\n",
        "\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "# 初期設定\n",
        "device = 'cuda'\n",
        "latent_dim = 8\n",
        "n_mlp = 5\n",
        "num_down = 3\n",
        "\n",
        "G_A2B = Generator(256, 4, latent_dim, n_mlp, channel_multiplier=1, lr_mlp=.01,n_res=1).to(device).eval()\n",
        "\n",
        "ensure_checkpoint_exists('GNR_checkpoint.pt')\n",
        "ckpt = torch.load('GNR_checkpoint.pt', map_location=device)\n",
        "\n",
        "G_A2B.load_state_dict(ckpt['G_A2B_ema'])\n",
        "\n",
        "# mean latent\n",
        "truncation = 1\n",
        "with torch.no_grad():\n",
        "    mean_style = G_A2B.mapping(torch.randn([1000, latent_dim]).to(device)).mean(0, keepdim=True)\n",
        "\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), inplace=True)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGoWzIYHEI4w"
      },
      "source": [
        "# 画像から生成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixXP3I5OifYg"
      },
      "source": [
        "**画像から顔画像の切り出し**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZseerVYDpYH"
      },
      "source": [
        "# 画像から顔画像を切り出す\n",
        "image = cv2.imread('samples/001.jpg')  # 画像指定\n",
        "height, width = image.shape[:2]\n",
        "\n",
        "# Detect with dlib\n",
        "face_detector = dlib.get_frontal_face_detector()\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "# grab first face\n",
        "face = face_detector(gray, 1)[0]\n",
        "\n",
        "# Face crop with dlib and bounding box scale enlargement\n",
        "x, y, size = get_boundingbox(face, width, height)\n",
        "cropped_face = image[y:y+size, x:x+size]\n",
        "cropped_face = cv2.cvtColor(cropped_face, cv2.COLOR_BGR2RGB)\n",
        "cropped_face = Image.fromarray(cropped_face)\n",
        "cropped_face"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO35tL5PP4ry"
      },
      "source": [
        "**複数のスタイル表示**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9k-eKy2pP8FS"
      },
      "source": [
        "%matplotlib inline\n",
        "plt.rcParams['figure.dpi'] = 200\n",
        "\n",
        "torch.manual_seed(84986)\n",
        "\n",
        "num_styles = 5\n",
        "style = torch.randn([num_styles, latent_dim]).to(device)\n",
        "\n",
        "\n",
        "# real_A = Image.open('./samples/margot_robbie.jpg')\n",
        "real_A = cropped_face\n",
        "real_A = test_transform(real_A).unsqueeze(0).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    A2B_content, _ = G_A2B.encode(real_A)\n",
        "    fake_A2B = G_A2B.decode(A2B_content.repeat(num_styles,1,1,1), style)\n",
        "    A2B = torch.cat([real_A, fake_A2B], 0)\n",
        "\n",
        "display_image(utils.make_grid(A2B.cpu(), normalize=True, range=(-1, 1), nrow=10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqdGkznnQBWw"
      },
      "source": [
        "**２つのスタイル補完**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SnBuXtzQAK2"
      },
      "source": [
        "torch.manual_seed(13421)\n",
        "\n",
        "real_A = cropped_face \n",
        "real_A = test_transform(real_A).unsqueeze(0).to(device)\n",
        "\n",
        "style1 = G_A2B.mapping(torch.randn([1, latent_dim]).to(device))\n",
        "style2 = G_A2B.mapping(torch.randn([1, latent_dim]).to(device))\n",
        "\n",
        "with torch.no_grad():\n",
        "    A2B = []\n",
        "    A2B_content, _ = G_A2B.encode(real_A)\n",
        "    for i in np.linspace(0,1,5):\n",
        "        new_style = i*style1 + (1-i)*style2\n",
        "        fake_A2B = G_A2B.decode(A2B_content, new_style, use_mapping=False)\n",
        "        A2B.append(torch.cat([fake_A2B], 0))\n",
        "    A2B = torch.cat([real_A] + A2B, 0)\n",
        "\n",
        "display_image(utils.make_grid(A2B.cpu(), normalize=True, range=(-1, 1), nrow=10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-jkcxR2QVNA"
      },
      "source": [
        "# ビデオから生成\n",
        "入力された顔のビデオを指定して、４種類のビデオグリッドを作成します。\n",
        "\n",
        "＊dlib顔検出を使用して、顔の周囲に境界ボックスを形成します。 バウンディングボックスは、最初のフレームで検出された最初の顔に基づいています。 したがって、ビデオの顔はあまり動かないようにする必要があります。そうしないと、バウンディングボックスから外れる可能性があります。\n",
        "\n",
        "・ビデオ生成には4つのモードがあります\\\n",
        "**1)normal** ： 異なるスタイルを継続使用\\\n",
        "**2)blend** ： 時間とともにスタイルを補間\\\n",
        "**3)beat** ： 音楽のビートに応じてスタイルを変化\\\n",
        "**4)eig** ： 音楽のビートに応じてスタイルを変化（固有ベクトルを使用）\\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYFG5LjsQTOX"
      },
      "source": [
        "import random\n",
        "import scipy.ndimage\n",
        "\n",
        "# input video\n",
        "inpath = './samples/satomi.mp4'  # ビデオ指定\n",
        "outpath = './samples/output.mp4'\n",
        "\n",
        "mode = 'beat'  # モード選択\n",
        "assert mode in ('normal', 'blend', 'beat', 'eig')\n",
        "\n",
        "\n",
        "# Frame numbers and length of output video\n",
        "start_frame=0\n",
        "end_frame=None\n",
        "frame_num = 0\n",
        "mp4_fps= 30\n",
        "faces = None\n",
        "smoothing_sec=.7\n",
        "eig_dir_idx = 1 # first eig isnt good so we skip it\n",
        "\n",
        "frames = []\n",
        "reader = cv2.VideoCapture(inpath)\n",
        "num_frames = int(reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "# get beats from audio\n",
        "win_s = 512                 # fft size\n",
        "hop_s = win_s // 2          # hop size\n",
        "\n",
        "s = source(inpath, 0, hop_s)\n",
        "samplerate = s.samplerate\n",
        "o = tempo(\"default\", win_s, hop_s, samplerate)\n",
        "delay = 4. * hop_s\n",
        "# list of beats, in samples\n",
        "beats = []\n",
        "\n",
        "# total number of frames read\n",
        "total_frames = 0\n",
        "while True:\n",
        "    samples, read = s()\n",
        "    is_beat = o(samples)\n",
        "    if is_beat:\n",
        "        this_beat = int(total_frames - delay + is_beat[0] * hop_s)\n",
        "        beats.append(this_beat/ float(samplerate))\n",
        "    total_frames += read\n",
        "    if read < hop_s: break\n",
        "#print len(beats)\n",
        "beats = [math.ceil(i*mp4_fps) for i in beats]\n",
        "\n",
        "\n",
        "if mode == 'blend':\n",
        "    shape = [num_frames, 8, latent_dim] # [frame, image, channel, component]\n",
        "    #all_latents = random_state.randn(*shape).astype(np.float32)\n",
        "    all_latents = np.random.randn(*shape).astype(np.float32)\n",
        "    all_latents = scipy.ndimage.gaussian_filter(all_latents, [smoothing_sec * mp4_fps, 0, 0], mode='wrap')\n",
        "    all_latents /= np.sqrt(np.mean(np.square(all_latents)))\n",
        "    all_latents = torch.from_numpy(all_latents).to(device)\n",
        "else:\n",
        "    all_latents = torch.randn([8, latent_dim]).to(device)\n",
        "    \n",
        "if mode == 'eig':\n",
        "    all_latents = G_A2B.mapping(all_latents)\n",
        "    \n",
        "in_latent = all_latents\n",
        "\n",
        "# Face detector\n",
        "face_detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "assert start_frame < num_frames - 1\n",
        "end_frame = end_frame if end_frame else num_frames\n",
        "\n",
        "while reader.isOpened():\n",
        "    _, image = reader.read()\n",
        "    if image is None:\n",
        "        break\n",
        "\n",
        "    if frame_num < start_frame:\n",
        "        continue\n",
        "    # Image size\n",
        "    height, width = image.shape[:2]\n",
        "\n",
        "    # 2. Detect with dlib\n",
        "    if faces is None:\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        faces = face_detector(gray, 1)\n",
        "    if len(faces):\n",
        "        # For now only take biggest face\n",
        "        face = faces[0]\n",
        "\n",
        "    # --- Prediction ---------------------------------------------------\n",
        "    # Face crop with dlib and bounding box scale enlargement\n",
        "    x, y, size = get_boundingbox(face, width, height)\n",
        "    cropped_face = image[y:y+size, x:x+size]\n",
        "    cropped_face = cv2.cvtColor(cropped_face, cv2.COLOR_BGR2RGB)\n",
        "    cropped_face = Image.fromarray(cropped_face)\n",
        "    frame = test_transform(cropped_face).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        A2B_content, A2B_style = G_A2B.encode(frame)\n",
        "        if mode == 'blend':\n",
        "            in_latent = all_latents[frame_num]\n",
        "        elif mode == 'normal':\n",
        "            in_latent = all_latents\n",
        "        elif mode == 'beat':\n",
        "            if frame_num in beats:\n",
        "                in_latent = torch.randn([8, latent_dim]).to(device)\n",
        "        \n",
        "        if mode == 'eig':\n",
        "            if frame_num in beats:\n",
        "                direction = 3 * eigvec[:, eig_dir_idx].unsqueeze(0).expand_as(all_latents).to(device)\n",
        "                in_latent = all_latents + direction\n",
        "                eig_dir_idx += 1\n",
        "                \n",
        "            fake_A2B = G_A2B.decode(A2B_content.repeat(8,1,1,1), in_latent, use_mapping=False)\n",
        "        else:\n",
        "            fake_A2B = G_A2B.decode(A2B_content.repeat(8,1,1,1), in_latent)\n",
        "\n",
        "        \n",
        "        \n",
        "        fake_A2B = torch.cat([fake_A2B[:4], frame, fake_A2B[4:]], 0)\n",
        "\n",
        "        fake_A2B = utils.make_grid(fake_A2B.cpu(), normalize=True, range=(-1, 1), nrow=3)\n",
        "\n",
        "\n",
        "    #concatenate original image top\n",
        "    fake_A2B = fake_A2B.permute(1,2,0).cpu().numpy()\n",
        "    frames.append(fake_A2B*255)\n",
        "\n",
        "    frame_num += 1\n",
        "        \n",
        "clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(frames, fps=mp4_fps)\n",
        "\n",
        "# save to temporary file. hack to make sure ffmpeg works\n",
        "clip.write_videofile('./temp.mp4')\n",
        "\n",
        "# use ffmpeg to add audio to video\n",
        "!ffmpeg -i ./temp.mp4 -i $inpath -c copy -map 0:v:0 -map 1:a:0 $outpath -y\n",
        "!rm ./temp.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TotFa5dcQbgc"
      },
      "source": [
        "mp4 = open(outpath,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}